{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HiggsBoson.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM2dBxpBnyqNL5chI2q8s1o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AritraStark/Higgs-Boson-Detection/blob/main/HiggsBoson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Detect Higgs Boson Particles from collision data:**\n",
        "\n",
        "The collision of protons at high energy can produce new particles like the Higgs boson. These particles can't be directly observed, however, since they decay almost instantly. So to detect the presence of a new particle, we instead observe the behavior of the particles they decay into, their \"decay products\".\n",
        "The Higgs dataset contains 21 \"low-level\" features of the decay products and also 7 more \"high-level\" features derived from these."
      ],
      "metadata": {
        "id": "fz1VXPeAojQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup and initialization of TPUs"
      ],
      "metadata": {
        "id": "_WrVL4Who72G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from kaggle_datasets import KaggleDatasets\n",
        "from tensorflow.io import FixedLenFeature\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import callbacks\n",
        "\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "# Detecting TPUs\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "except ValueError:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n",
        "\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "plt.rc('figure', autolayout=True)\n",
        "plt.rc('axes', labelweight='bold', labelsize='large',\n",
        "       titleweight='bold', titlesize=18, titlepad=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uA7fJalzo-8t",
        "outputId": "32eae006-22cf-4cb8-f269-828fa5802bdd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version 2.8.0\n",
            "Number of accelerators:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading dataset and initializing hyper-parameters"
      ],
      "metadata": {
        "id": "PZurgYOdpWn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "UNITS = 2 ** 11\n",
        "ACTIVATION = 'relu'\n",
        "DROPOUT = 0.1\n",
        "\n",
        "#(Power of 128)\n",
        "BATCH_SIZE_PER_REPLICA = 2 ** 11\n",
        "\n",
        "def make_decoder(feature_description):\n",
        "    def decoder(example):\n",
        "        example = tf.io.parse_single_example(example, feature_description)\n",
        "        features = tf.io.parse_tensor(example['features'], tf.float32)\n",
        "        features = tf.reshape(features, [28])\n",
        "        label = example['label']\n",
        "        return features, label\n",
        "    return decoder\n",
        "\n",
        "\n",
        "def load_dataset(filenames, decoder, ordered=False):\n",
        "    AUTO = tf.data.experimental.AUTOTUNE\n",
        "    ignore_order = tf.data.Options()\n",
        "    if not ordered:\n",
        "        ignore_order.experimental_deterministic = False\n",
        "    dataset = (\n",
        "        tf.data\n",
        "        .TFRecordDataset(filenames, num_parallel_reads=AUTO)\n",
        "        .with_options(ignore_order)\n",
        "        .map(decoder, AUTO)\n",
        "    )\n",
        "    return dataset\n",
        "  \n",
        "dataset_size = int(11e6)\n",
        "validation_size = int(5e5)\n",
        "training_size = dataset_size - validation_size\n",
        "\n",
        "\n",
        "# For model.fit\n",
        "batch_size = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
        "steps_per_epoch = training_size // batch_size\n",
        "validation_steps = validation_size // batch_size\n",
        "\n",
        "\n",
        "# For model.compile\n",
        "steps_per_execution = 256\n",
        "feature_description = {\n",
        "    'features': FixedLenFeature([], tf.string),\n",
        "    'label': FixedLenFeature([], tf.float32),\n",
        "}\n",
        "\n",
        "decoder = make_decoder(feature_description)\n",
        "\n",
        "data_dir = KaggleDatasets().get_gcs_path('higgs-boson')\n",
        "train_files = tf.io.gfile.glob(data_dir + '/training' + '/*.tfrecord')\n",
        "valid_files = tf.io.gfile.glob(data_dir + '/validation' + '/*.tfrecord')\n",
        "\n",
        "ds_train = load_dataset(train_files, decoder, ordered=False)\n",
        "ds_train = (\n",
        "    ds_train\n",
        "    .cache()\n",
        "    .repeat()\n",
        "    .shuffle(2 ** 19)\n",
        "    .batch(batch_size)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "ds_valid = load_dataset(valid_files, decoder, ordered=False)\n",
        "ds_valid = (\n",
        "    ds_valid\n",
        "    .batch(batch_size)\n",
        "    .cache()\n",
        "    .prefetch(AUTO)\n",
        ")"
      ],
      "metadata": {
        "id": "P4foXltApYC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the model"
      ],
      "metadata": {
        "id": "SRyF8cqIpZdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dense_block(units, activation, dropout_rate, l1=None, l2=None):\n",
        "    def make(inputs):\n",
        "        x = layers.Dense(units)(inputs)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation(activation)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "        return x\n",
        "    return make\n",
        "\n",
        "\n",
        "with strategy.scope():\n",
        "    # Wide Network\n",
        "    wide = keras.experimental.LinearModel()\n",
        "\n",
        "    # Deep Network\n",
        "    inputs = keras.Input(shape=[28])\n",
        "    x = dense_block(UNITS, ACTIVATION, DROPOUT)(inputs)\n",
        "    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n",
        "    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n",
        "    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n",
        "    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n",
        "    outputs = layers.Dense(1)(x)\n",
        "    deep = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    \n",
        "    # Wide and Deep Network\n",
        "    wide_and_deep = keras.experimental.WideDeepModel(\n",
        "        linear_model=wide,\n",
        "        dnn_model=deep,\n",
        "        activation='sigmoid',\n",
        "    )\n",
        "\n",
        "wide_and_deep.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['AUC', 'binary_accuracy'],\n",
        "    experimental_steps_per_execution=steps_per_execution,\n",
        ")"
      ],
      "metadata": {
        "id": "BO0pE4gGp2KF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and fitting the data:"
      ],
      "metadata": {
        "id": "Gc-caYt0p4jh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = callbacks.EarlyStopping(\n",
        "    patience=2,\n",
        "    min_delta=0.001,\n",
        "    restore_best_weights=True,\n",
        ")\n",
        "\n",
        "lr_schedule = callbacks.ReduceLROnPlateau(\n",
        "    patience=0,\n",
        "    factor=0.2,\n",
        "    min_lr=0.001,\n",
        ")\n",
        "history = wide_and_deep.fit(\n",
        "    ds_train,\n",
        "    validation_data=ds_valid,\n",
        "    epochs=50,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_steps=validation_steps,\n",
        "    callbacks=[early_stopping, lr_schedule],\n",
        ")"
      ],
      "metadata": {
        "id": "B6_bjjwBp-Hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing the accuracy and loss plots"
      ],
      "metadata": {
        "id": "AUM6dPEgqLzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_frame = pd.DataFrame(history.history)\n",
        "history_frame.loc[:, ['loss', 'val_loss']].plot(title='Cross-entropy Loss')\n",
        "history_frame.loc[:, ['auc', 'val_auc']].plot(title='AUC');"
      ],
      "metadata": {
        "id": "tr4YIeYYqQhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "\n",
        "\n",
        "*   [Baldi, P. et al. Searching for Exotic Particles in High-Energy Physics with Deep Learning. (2014)](https://arxiv.org/abs/1402.4735)\n",
        "*  [Cheng, H. et al. Wide & Deep Learning for Recommender Systems. (2016)](https://arxiv.org/abs/1606.07792)\n",
        "\n"
      ],
      "metadata": {
        "id": "sdySdkTrqS9q"
      }
    }
  ]
}